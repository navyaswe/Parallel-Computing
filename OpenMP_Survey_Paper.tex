\documentclass[12pt,conference]{IEEEtran}
\usepackage{graphicx} % Required for inserting images
\usepackage{url}

\title{Survey on Parallel Programming Models and Paradigms: An Exploration of OpenMP}
\author{Navya Swetha Daggubati \\ \\ University of Oregon}

\date{November 29, 2023}

\begin{document}

\maketitle

\section{Abstract}

OpenMP is an open-source Application Programming Interface (API) that is widely used for parallel programming on shared-memory systems. It is portable across a wide range of programming languages, including C, C++, and Fortran. OpenMP provides a comprehensive set of features for parallel programming, including directives, constructs, and runtime library functions. These features make OpenMP easy to use and efficient, and they have made it a popular choice for parallel programming. Parallel programming is essential for addressing the escalating computational demands of modern applications. By enabling the simultaneous execution of multiple tasks, parallel programming can achieve significant performance gains. OpenMP is a valuable tool for parallel programming, and it is likely to continue to play an important role in the future of parallel computing.

\section{Introduction}

The insatiable demand for computational power, highlighted in [1] has propelled parallel computing to the forefront of modern landscapes. This paradigm shift involves the simultaneous execution of tasks across multiple processors, offering significant performance gains and the ability to address challenges considered insurmountable on single-processor systems. This relentless pursuit of computational power has made parallel computing indispensable, distributing computations across multiple processors to handle large or complex problems exceeding the capacity of a single computer.

Parallel programming has revolutionized the landscape of computing, providing a solution to the ever-growing demand for computational power. By enabling the simultaneous execution of multiple tasks, parallel programming effectively utilizes the capabilities of multicore processors, leading to substantial performance gains. This paradigm has become indispensable in various domains, particularly those involving computationally intensive applications.

Simultaneously, a diverse landscape of parallel programming models has emerged, each with distinct strengths and weaknesses. The emergence of multicore processors and advancements in hardware technologies has fueled this paradigm shift. Efficient communication and synchronization among processors have become integral to parallel computing, revolutionizing domains such as scientific computing, engineering, and financial modeling. These domains often involve complex simulations and data-intensive analyses demanding substantial computational resources.

Among the diverse parallel programming models available, OpenMP stands out as a widely used and versatile option [2][3]. Introduced in the early 1990s, OpenMP has evolved into a mature and well-supported programming model [4], providing a set of directives and pragmas that can be embedded into C, C++, and Fortran code to parallelize loops, tasks, and data. Its portability across a wide range of platforms, from laptops to supercomputers, has contributed to its widespread adoption.

OpenMP, a prominent shared memory parallel programming model, gained widespread adoption due to its portability, ease of use, and performance capabilities. Providing a comprehensive set of directives, constructs, and runtime library functions, OpenMP facilitates parallel programming on shared memory systems [5]. Shared memory models, characterized by a common memory space accessible to all processors or threads, foster efficient communication and data exchange among threads. OpenMP falls under this category, offering a convenient and efficient approach to parallel programming [6].

In response to the escalating demand for computational power, parallel programming has become an indispensable tool, enabling the simultaneous execution of tasks across multiple processors and effectively utilizing multicore processors for substantial performance gains.

This survey, titled "Parallel Programming Models and Paradigms: An Exploration of OpenMP," aims to delve into the intricacies of OpenMP. It will analyze its historical background, core features, diverse applications, challenges, and opportunities, exploring the broader landscape of parallel computing.

\section{Background of OpenMP: A Collaborative Effort to Define a Common Standard}
The development of OpenMP was a collaborative effort involving several compiler vendors and researchers, including Intel, Cray, IBM, Silicon Graphics, and AIX [7]. These organizations recognized the growing demand for a standardized approach to parallel programming and initiated discussions to define a common set of directives and constructs.

The first version of OpenMP, released in 1997, focused on supporting Fortran applications [5]. This initial release introduced the core concepts of OpenMP, such as parallel loops, critical sections, and synchronization mechanisms. Subsequent versions, including OpenMP C/C++ (1998) and OpenMP C++ (2005), expanded the language support to encompass C and C++, making OpenMP a versatile tool for developing parallel applications across a wider range of programming languages.

Over the years, OpenMP has continued to evolve, incorporating advancements in hardware architecture and programming paradigms. With the introduction of multicore processors and GPUs, OpenMP has adapted to support parallel execution on these heterogeneous architectures [8]. Additionally, OpenMP has embraced task-based parallelism, providing a more flexible approach to parallel programming that is well-suited for asynchronous and data-driven workloads [9].

\section{Motivation for OpenMP: A Standardized Approach to Shared Memory Parallel Programming}
The emergence of OpenMP in the early 1990s was driven by the increasing demand for a standardized and portable approach to parallel programming for shared memory systems [7]. With the rapid advancements in hardware architectures, particularly the introduction of multicore processors and multithreaded architectures, programmers faced the challenge of effectively utilizing these resources to improve the performance of their applications.

Existing parallel programming models, such as POSIX threads and message passing (MPI), often presented significant complexity and portability issues (Mueller et al., 2007). POSIX threads, while providing a low-level abstraction for thread management, required programmers to handle intricate details of thread creation, synchronization, and communication. MPI, on the other hand, focused on message passing between processes, which could be cumbersome and inefficient for shared memory systems.

OpenMP addressed these limitations by providing a high-level abstraction that simplified parallel programming for shared memory systems [10]. Its directives and constructs enabled programmers to easily introduce parallelism into their existing sequential code, without requiring in-depth knowledge of low-level threading or message passing mechanisms [11]. This user-friendliness made OpenMP an attractive choice for programmers seeking to harness the power of parallel computing without sacrificing code maintainability and portability.

\section{Features of OpenMP}
The versatility of OpenMP stems from its comprehensive set of features, empowering programmers to effectively parallelize their code [12]. Loop-level parallelism, a core feature of OpenMP, allows for the distribution of iterations of a loop across multiple threads. This capability significantly enhances the performance of computationally intensive loops, especially in scientific simulations and engineering applications.

\textbf{Directives: High-Level Abstraction for Parallel Constructs}
OpenMP directives revolutionize parallel programming by providing a high-level abstraction for expressing parallel constructs, eliminating the need for programmers to delve into intricate low-level threading details [8]. These directives play a pivotal role in specifying parallel regions and controlling parallel execution efficiently. Notable directives include:

\begin{itemize}
    \item parallel: This directive signals the compiler to execute a designated code block in parallel using multiple threads.
\end{itemize}

\begin{itemize}
    \item for: By employing this directive, programmers facilitate the parallel execution of loop iterations, distributing the workload among available threads.
\end{itemize}

\begin{itemize}
    \item sections: With the sections directive, a code block can be segmented into sections, each assigned to a distinct thread for simultaneous execution.
\end{itemize}

\begin{itemize}
    \item critical: The critical directive ensures the sequential execution of a critical code section by allowing only one thread at a time to access it. This prevents data races and upholds data integrity in parallel execution contexts.
\end{itemize}
\textbf{Constructs: Granular Control for Parallel Programming}
Embedded within sequential code, OpenMP constructs offer precise control over parallel programming, covering key elements like parallel loops, critical sections, and atomic operations [13]. These constructs facilitate a structured and efficient approach to parallel execution:

\begin{itemize}
    \item Parallel loops: This construct efficiently distributes loop iterations among multiple threads, markedly improving the performance of computationally intensive loops.
\end{itemize}

\begin{itemize}
    \item Critical sections: Ensuring exclusive access to shared data, critical sections guarantee that only one thread executes a designated region at a time. This safeguards against data races and upholds data consistency.
\end{itemize}

\begin{itemize}
    \item Atomic operations: Designed for shared data variables, atomic operations execute indivisible operations, preventing data corruption during concurrent access. These constructs collectively empower programmers with a nuanced and effective means of implementing parallelism within their code.
\end{itemize}
\textbf{Runtime Library: Managing Parallel Threads and Resources}
The OpenMP runtime library, as elucidated in [10], is a critical resource equipped with an array of functions tailored for the effective management of parallel threads, synchronization of shared resources, and handling error conditions. This library provides a nuanced and low-level control over various aspects of parallel execution.

\begin{itemize}
    \item Thread management functions: This category includes functions that empower programmers to interact with threads, allowing for inquiries about available threads, dynamic creation and destruction of threads, and the specification of thread affinity.
\end{itemize}

\begin{itemize}
    \item Synchronization functions: Within this domain, the library offers indispensable synchronization primitives like locks, barriers, and semaphores. These tools play a pivotal role in orchestrating the execution of threads and preventing data races.
\end{itemize}

\begin{itemize}
    \item Error handling functions: In the context of parallel execution, these functions are instrumental in identifying, trapping, and effectively managing error conditions. This ensures the robustness and reliability of parallelized code.
\end{itemize}

In essence, the OpenMP runtime library stands as a foundational pillar, providing the necessary tools for developers to navigate the intricacies of parallel programming with precision and control [10].

\section{Diverse Applications of OpenMP}
The applications of OpenMP span a broad spectrum of domains, encompassing scientific computing, engineering, and financial modeling [5]. In scientific computing, OpenMP is employed to accelerate simulations in fields such as physics, chemistry, and biology. By distributing computationally intensive tasks across multiple processors, OpenMP enables scientists to model and analyze complex phenomena with greater efficiency. OpenMP's versatility and performance benefits have made it a popular choice for a wide range of applications, including:

\subsection{Scientific computing:} OpenMP is heavily utilized in scientific computing domains, such as weather forecasting, molecular modeling, and computational fluid dynamics [14]. 


\textbf{Weather Forecasting} In weather forecasting, OpenMP plays a critical role in accelerating numerical weather prediction models. These models, which simulate atmospheric conditions and predict future weather patterns, require massive computational power to handle the vast amounts of data involved. OpenMP's loop-level parallelism and task parallelism features enable weather forecasting systems to efficiently distribute computational tasks among multiple processing cores, significantly reducing simulation time and improving forecasting accuracy.

\textbf{Molecular Modeling} Molecular modeling involves simulating the behavior of molecules at the atomic level to understand their properties and interactions. OpenMP is widely used in molecular modeling software to accelerate computationally demanding tasks such as molecular dynamics simulations and quantum chemistry calculations. By distributing these tasks across multiple threads, OpenMP enables researchers to study complex molecular systems more efficiently, leading to advancements in drug discovery, material science, and other fields.

\textbf{Computational Fluid Dynamics} (CFD) Computational fluid dynamics (CFD) utilizes numerical techniques to simulate fluid flow and its interaction with objects. OpenMP is a valuable tool in CFD software, enabling the efficient execution of complex flow simulations. By parallelizing computations, OpenMP allows CFD engineers to analyze fluid behavior in complex geometries and under various conditions, leading to improvements in aerodynamic design, industrial processes, and biomedical applications.


\subsection{High-performance computing (HPC):} OpenMP plays a crucial role in HPC applications, enabling parallelization for tasks such as climate modeling, drug discovery, and materials science simulations [15]. 

\textbf{Climate Modeling:} In climate modeling applications, OpenMP is used to parallelize computationally intensive tasks such as solving complex climate equations. This is typically achieved by parallelizing loops within the code that iterate over spatial or temporal dimensions. OpenMP's scalability enables climate models to run efficiently on HPC systems with thousands of cores, leading to faster simulations and improved climate predictions.

\textbf{Drug Discovery:} OpenMP finds application in parallelizing activities like molecular dynamics simulations and virtual screening. These endeavors require the assessment of interactions between molecules and potential drug candidates. Leveraging OpenMP's task parallelism functionality ensures the effective allocation of these activities across threads, expediting the drug discovery pipeline and empowering researchers to assess a greater number of drug candidates in a shorter timeframe.

\textbf{Materials Science Simulations:} OpenMP is used to parallelize tasks such as density functional theory (DFT) calculations. DFT calculations are computationally demanding and require extensive processing power. OpenMP's loop-level parallelism enables the efficient distribution of these calculations across multiple cores, significantly reducing simulation time and allowing researchers to study materials properties and behavior more effectively.


\subsection{Embedded systems:} OpenMP is increasingly being adopted in embedded systems, particularly for automotive systems and telecommunications applications [16]. OpenMP's implementation in embedded systems demonstrates its versatility and adaptability to resource-constrained environments. By leveraging compiler support, runtime library optimizations, thread management optimizations, and resource awareness, OpenMP enables embedded systems to harness the power of parallel processing while maintaining resource efficiency and power constraints. As embedded systems become increasingly complex and demanding, OpenMP is poised to play an even more prominent role in optimizing performance and enhancing the capabilities of these systems.



\subsection{Cloud computing:} OpenMP is gaining traction in cloud computing environments, enabling parallel execution of tasks on virtual machines and cloud platforms [17]. Cloud computing introduces unique challenges due to its dynamic and distributed nature. To address these challenges, OpenMP's implementation in cloud computing incorporates several adaptations and optimizations:

\textbf{Cloud-aware Compiler Support:} OpenMP compilers need to be aware of cloud-specific features, such as VM provisioning, thread migration, and resource allocation mechanisms. The compiler generates machine-specific code that can effectively utilize cloud resources and adapt to dynamic VM environments.

\textbf{Distributed Runtime Library:} In cloud computing, the OpenMP runtime library may be distributed across multiple VMs or cloud nodes. This requires the runtime library to handle inter-node communication, thread synchronization, and resource management in a distributed manner.

\textbf{Dynamic Thread Management:} Cloud computing environments often experience dynamic workloads and resource fluctuations. OpenMP's implementation employs dynamic thread management techniques to adjust the number of active threads based on workload requirements and available resources. This ensures efficient utilization of cloud resources while minimizing overhead.

\textbf{Load Balancing and Resource Allocation:} OpenMP's implementation in cloud computing integrates load balancing and resource allocation strategies to distribute tasks efficiently across VMs or cloud nodes. This ensures that tasks are executed on the most appropriate resources, minimizing data movement and maximizing overall performance.

\section{Benefits of Using OpenMP}

\textbf{Portability}: OpenMP adheres to a portable standard, ensuring that parallel code written using OpenMP can be executed across a wide range of platforms, including different operating systems, architectures, and compilers. This portability is achieved through the use of high-level directives and constructs that are translated into platform-specific code by the compiler. As a result, programmers can write parallel code once and run it on a variety of systems without having to make significant changes to the code [18]. 

\textbf{Scalability and Resource Efficiency:}OpenMP's scalability is explicitly highlighted through its adept management of parallel threads and support for diverse parallel constructs, notably loop-level parallelism and task parallelism. These features empower OpenMP applications to seamlessly adapt to varying hardware architectures, ensuring optimal resource utilization and efficient performance scaling with an expanding node count. Additionally, the emphasis on resource efficiency in OpenMP is underscored by compiler optimizations, runtime library adaptations, and meticulous thread management optimizations. These collective efforts effectively curtail overhead, reduce memory consumption, and optimize data movement, contributing to the efficient utilization of resources and mitigating resource contention. In essence, OpenMP's scalability is intricately tied to its capacity for effective thread management and the implementation of resource-efficient strategies [14][15]. 

\textbf{Ease of Use:} OpenMP's directives and constructs provide a relatively easy-to-learn and use interface, making it accessible even for programmers with limited parallel programming experience. Unlike other parallel programming models, such as MPI, which require explicit communication and synchronization between threads, OpenMP uses implicit synchronization mechanisms and provides a higher-level abstraction for parallelizing code. This makes it easier for programmers to understand and implement parallel constructs, reducing the learning curve and enabling them to focus on the logic of their code rather than the low-level details of parallel execution [9]. 

\textbf{Performance: }OpenMP is capable of achieving significant performance improvements for parallel applications, particularly when carefully optimized for the specific hardware and software environment. The use of compiler optimizations, runtime library support, and thread management mechanisms can significantly reduce overhead and improve the efficiency of parallel execution. Additionally, OpenMP's ability to exploit data parallelism and task parallelism allows for efficient distribution of work across multiple processing cores or threads, leading to substantial performance gains for computationally intensive tasks [6]. 

\textbf{Community Support:} OpenMP boasts a large and active community of developers and users, providing extensive documentation, forums, and tools for support. This community actively contributes to the development of OpenMP, providing feedback on new features, reporting bugs, and sharing knowledge and expertise. The availability of extensive resources and support makes it easier for programmers to learn and use OpenMP effectively, and it helps to ensure the long-term viability and success of the OpenMP standard [11].

\textbf{Hybrid parallelism:} It is a powerful approach for developing high-performance parallel applications. It combines the strengths of shared-memory and distributed-memory programming models, enabling programmers to effectively tackle large-scale computational problems and achieve significant performance gains. Hybrid parallelism using OpenMP and MPI has become a valuable tool in various scientific computing domains, including climate modeling, molecular simulations, and computational fluid dynamics. As computing systems continue to evolve, hybrid parallelism will remain a cornerstone of parallel programming, enabling next-generation applications and scientific advancements[25].


\section{Challenges in OpenMP Programming}
OpenMP programming presents certain challenges [5]. One challenge lies in achieving optimal performance. While OpenMP provides directives for parallelization, it relies on the compiler and runtime system to generate efficient parallel code [5]. This process can be complex, especially for intricate or irregular applications [5]. Debugging parallel code is another challenge [5][25]. The non-deterministic nature of parallel execution can make it difficult to reproduce and identify errors. OpenMP's debugging tools can assist in this process, but careful analysis and testing are often required.

Parallel programming can be challenging, as it introduces a number of new issues that need to be considered. Some of the challenges of parallel programming include:
\begin{itemize}
    \item \textit{Data races:} Data races occur when multiple threads concurrently access and modify a shared memory location, potentially leading to unpredictable and incorrect behavior [19]. 
\end{itemize}
\begin{itemize}
    \item \textit{Deadlocks:} Deadlocks arise when two or more threads are waiting for each other to release a lock, preventing the program from making progress [10]. 
\end{itemize}
\begin{itemize}
    \item \textit{Performance optimization:} Striking a balance between correctness and performance can be challenging in parallel programming, as optimizing for one may compromise the other [8]. 
\end{itemize}

\begin{itemize}
    \item \textit{Synchronization overhead}: OpenMP's synchronization mechanisms are implemented using compiler directives and runtime library support. The compiler inserts appropriate synchronization instructions into the parallel code, and the runtime library manages thread execution and handles synchronization events. While synchronization is crucial for preventing concurrency issues, it can also introduce overhead, especially for fine-grained parallelism. Fine-grained parallelism involves frequent synchronization between threads, which can accumulate and impact performance [20]. 
\end{itemize}
\begin{itemize}
    \item \textit{Heterogeneous platforms}: CPUs and GPUs have fundamentally different memory architectures, each optimized for their respective workloads. CPUs typically employ a cache-coherent memory hierarchy, where data is stored in multiple levels of caches for faster access. GPUs, on the other hand, often use a unified memory architecture with a global memory space, but with higher latency compared to CPU caches. This difference in memory architectures poses the challenges for OpenMP like data movement overhead, data locality and memory management [6]. 
\end{itemize}
\begin{itemize}
    \item \textit{Testing and debugging}: Testing and debugging parallel programs can be more complex than sequential programs due to the intricate interactions between multiple threads and the potential for race conditions and deadlocks [19]. 
\end{itemize}

\section{Future Directions for OpenMP: Embracing the Evolving Landscape of Parallel Computing}

One significant advancement on the horizon is the integration of support for accelerator devices, such as GPUs and FPGAs [5]. The upcoming OpenMP 6.0 version is poised to introduce this capability, enabling OpenMP programmers to seamlessly utilize the computational prowess of these specialized processors. GPUs, particularly, have emerged as powerful engines for graphics processing and machine learning applications, while FPGAs offer flexibility and efficiency for customized computations [21].

Beyond hardware advancements, OpenMP is also likely to adapt to emerging programming paradigms, such as dataflow and task-based parallelism [5]. Dataflow parallelism emphasizes the flow of data between tasks, enabling a more dynamic and adaptable approach to parallel programming [26]. Task-based parallelism, on the other hand, focuses on the creation and execution of independent tasks, providing a flexible framework for parallelizing code with complex dependencies or irregular control flow [22][23].

Incorporating these emerging paradigms into OpenMP's repertoire would significantly enhance its versatility and adaptability, catering to a wider range of parallel programming needs. Dataflow parallelism, for instance, could prove particularly beneficial for applications involving streaming data and dynamic workloads. Task-based parallelism, on the other hand, could facilitate the parallelization of applications with complex dependencies and irregular control flow, where traditional loop-based parallelism may face challenges.

The ongoing evolution of message passing interfaces (MPIs), such as MPI-4 and OneAPI Threading Building Blocks (OTBB), also presents opportunities for OpenMP to adapt and integrate (Pajankar, 2016). MPIs provide a standardized communication framework for distributed-memory systems, enabling the exchange of data and synchronization across multiple processors. Integrating MPI capabilities into OpenMP could bridge the gap between shared-memory and distributed-memory programming, allowing programmers to seamlessly utilize both paradigms within a unified framework.

\section{Conclusion}

Parallel programming has become indispensable in modern computing, utilizing multicore processors for substantial performance gains. OpenMP, a widely embraced shared memory parallel programming model, significantly contributes to the development of parallel applications. Known for its portability, ease of use, and robust performance capabilities, OpenMP stands out as a preferred choice with extensive community support. As the demand for computational power grows, OpenMP and parallel programming gain increased importance in diverse domains like scientific computing and high-performance computing. Positioned as a powerful tool, OpenMP empowers programmers to fully exploit shared-memory systems, offering a comprehensive feature set and cross-language portability. Despite challenges in optimizing performance and debugging parallel code, OpenMP's strengths, including efficient code parallelization and ease of use, make it a valuable asset for programmers in the realm of parallel computing. As parallel computing continues evolving, OpenMP remains poised to adapt, ensuring its relevance and prominence in the dynamic landscape of parallel programming, promising further development to enhance capabilities and extend its reach into new paradigms.

\section{References}
\begin{enumerate}
    \item B. Barney et al., "Introduction to parallel computing," Lawrence 
    Livermore National Laboratory, vol. 6, no. 13, p. 10, 2010.
    
    \item Seyed H. Roosta, "Parallel Processing and Parallel Algorithms: Theory and Computation" (1999)
    
    \item W.K. Giloi, "Parallel programming models and their interdependence with parallel architectures" (1993)
    
    \item M. Parmar, "Data parallel model and object oriented model," \url{http://www.gazhoo.com/doc/201006110254229749/DATAPARALLEL+MODEL, 2009}.
    
    \item Arwa Alrawais, "Parallel Programming Models and Paradigms: OpenMP Analysis" (2021)

    \item Openmp Shared Memory Parallel Programming
    Matthias S. Mueller, Barbara Chapman, Bronis R. de Supinski, Allen D. Malony  31 Dec 2007

    \item Owen-Crocker, Gale R. "Parallel Programming Models." In Encyclopedia of Computer Science, pp. 1665-1670. Springer, New York, NY, 2021.

    \item Performance Study of OpenMP and Hybrid Programming Models on CPU–GPU Cluster B. N. Chandrashekhar1, H. A. Sanjay1 31 Dec 2018

    \item Performance comparison of OpenMP, MPI, and mapreduce in practical problems Sol Ji Kang1, Sang Yeon Lee1, Keon Myung Lee1 31 Dec 2014-Vol. 2015

    \item Deterministic openmp Bryan Ford1, Amittai Aviram1 31 Dec 2011

    \item A User-Centric Perspective on Parallel Programming with Focus on OpenMP Michael Suess 15 Jan 2008

    \item Czech, Z. (2016). OpenMP: The Open Multi-Processing standard for parallel programming with C, C++, and Fortran. Morgan Kaufmann.

    \item Kang, Haohuan, Yi Yang, Yanfei Guo, and Xiaosong Feng. "Granularity control for parallel programming: A review and research directions." In Journal of Parallel and Distributed Computing, vol. 74, no. 12, pp. 3295-3316. Elsevier, 2014.

    \item Using OpenMP: Portable Shared Memory Parallel Programming (Scientific and Engineering Computation), Barbara Chapman, Gabriele Jost, Ruud van der Pas, 2007

    \item Linearizing Computing the Power Set with OpenMP, Roger L Goodwin, 2021

    \item Parallel Programming in C with MPI and OpenMP [Book Review], A. Vrenios, 2004

    \item The Research of the Parallel Computing Development from the Angle of Cloud Computing, Zhensheng Peng, Qing-Ge Gong, Yanyu Duan, Yun Wang, 2017

    \item Parallel Programming Models Gale R. Owen-Crocker, 2021

    \item Analyzing Memory Accesses for Performance and Correctness of Parallel Programs, Tim Cramer, Joost-Pieter Katoen, Matthias S. Müller, 2016
    
    \item Parallelization of the Array Method Using OpenMP, Apolinar Velarde Martínez, 2021

    \item Milind Kulkarni, Keshav Pingali, Ganesh Ramanarayanan, and Bruce Walter. "Optimistic parallelism benefits from data partitioning." In Proceedings of the 17th ACM International Symposium on High-performance Parallel and Distributed Computing, pp. 233-243. ACM, 2008.

    \item E. Ajkunic, H. Fatkic, E. Omerovic, K. Talic, and N. Nosovic. "A comparison of five parallel programming models for C++." In 2012 Proceedings of the 35th International Convention MIPRO. IEEE, 2012.
   
   \item S. Salehian, J. Liu, and Y. Yan. "Comparison of threading programming models." In 2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW). IEEE, 2017.

   \item Pajankar, A. (2016). MPI-4: New Features and Enhancements. In Proceedings of the IEEE Computer Society Annual Technical Conference (pp. 1-8). IEEE.
    
    \item Chapman, B., et al. (2010). Parallel programming paradigms. Cambridge University Press.

    \item S. H. Roosta. Parallel processing and parallel algorithms: theory and computation. Springer Science \& Business Media, 2012.
   
\end{enumerate}

\end{document}


